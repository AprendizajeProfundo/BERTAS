{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b51d4c2-3e24-4a7f-8257-acbdc6965123",
   "metadata": {
    "tags": []
   },
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:blue\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd24c64-3e9a-4887-9739-42d8796e17c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <span style=\"color:red\"><center>BERTAS</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36e01c6-43e8-4fe4-8b41-da91f83b5577",
   "metadata": {},
   "source": [
    "<center>Creation of Embeddings</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c285115a-ff80-4943-9076-81cecef30437",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Autores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51820da-c0e3-4df5-8297-6f2d742b7d8d",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c9a186-3466-4d71-99ac-dcda6ddd5e41",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Referencias</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef6d570-6bf9-4285-8ed6-2b728af39c05",
   "metadata": {},
   "source": [
    "1. [HuggingFace. Transformers ](https://huggingface.co/transformers/)\n",
    "1. [HuggingFace. Intro pipeline](https://huggingface.co/course/chapter1/3?fw=pt)\n",
    "1. [Sentence transformer](https://github.com/UKPLab/sentence-transformers)\n",
    "1. [SBERT paper](https://arxiv.org/pdf/1908.10084.pdf)\n",
    "1. [SBERT.net](https://www.sbert.net/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94627548-4158-4901-a5f3-d1cbf7dbca14",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b5b782-51c9-4175-8aeb-c2e9da206d46",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee4e4bb-647d-4820-923c-c12e2ce69828",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Introduction</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab42dc6-7fc2-4c54-8f2d-b8aab561c5a3",
   "metadata": {},
   "source": [
    "In this notebook we read the sentence-transformer embeddings of the esays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2254769-d728-4aad-bf04-8c1fe08c6d1e",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Load required modules</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fd2b4c7-1615-463a-aa86-cd82172902f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "from glob import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc24a31e-d3b3-4671-80b7-8f2c87bc9440",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Open hdf5 file</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d972093-2f3c-4484-b64c-ef792db71d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crea el archivo hdf5\n",
    "# userblock for documentation\n",
    "# \n",
    "path_hdf5 = '../Datos/Embeddings_hdf5/'\n",
    "file_names_hdf5 = ['../Datos/Embeddings_hdf5/F1_1783.hdf5',\n",
    "                 '../Datos/Embeddings_hdf5/F2_1800.hdf5',\n",
    "                 '../Datos/Embeddings_hdf5/F3_1726.hdf5',\n",
    "                 '../Datos/Embeddings_hdf5/F4_1772.hdf5',\n",
    "                 '../Datos/Embeddings_hdf5/F5_1805.hdf5',\n",
    "                 '../Datos/Embeddings_hdf5/F6_1800.hdf5',\n",
    "                 '../Datos/Embeddings_hdf5/F7_1569.hdf5',\n",
    "                 '../Datos/Embeddings_hdf5/F8_723.hdf5']\n",
    "\n",
    "path_csv = '../Datos/Clean_data/'\n",
    "file_names_csv = sorted(glob(path_csv + 'F*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41798307-c88f-4e69-93ce-53f188b4405c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in file_names_hdf5:\n",
    "    f = h5py.File(file, 'w', userblock_size=512)\n",
    "    with open(file, 'r+') as f1:\n",
    "        doc='This example contains the embeddings of all the essays of an exam type./nThere are two objects:/nessay_id-> It contains the identifier of each essay./data-> It is an array of size num_total_essays x768 and they correspond to the embeddings of the sentences averaged per trial using the all-mpnet-base-v2 model of sentence transformers.'    \n",
    "        f1.write(doc)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff1ee14-0055-4ac7-b039-03db4109b53b",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Create sentence model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e79223f9-c95d-470c-9336-6a94cf26a501",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, LoggingHandler\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "import logging\n",
    "\n",
    "# prepare to parallel embeddings\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc92b17c-6e78-408c-b271-7cd124da3203",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Load and process data</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dae77e4-35d5-4530-9535-019264e11abd",
   "metadata": {},
   "source": [
    "1. Create a list with the file names.\n",
    "1. Process each file:\n",
    "    * read file\n",
    "    * extract the column sentence,\n",
    "    * compute number of sentences of each essay,\n",
    "    * transform all esaay in a unique list,\n",
    "    * process in parallel all sentences (embedding),\n",
    "    + save the embeddings in the hdf5 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c0a0c47-7ff2-490b-a319-99cd1fe2b398",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/bizon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') # Manejo de puntuación\n",
    "import matplotlib.pyplot as plt\n",
    "# tokenizers\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcb5344-7cc3-4ebd-acb2-f3b8fa497814",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# process the embedings \n",
    "if __name__ == '__main__':\n",
    "    #Start the multi-process pool on all available CUDA devices\n",
    "    pool = model.start_multi_process_pool()\n",
    "    \n",
    "    # process each file\n",
    "    for file_csv, file_hdf5 in zip(file_names_csv, file_names_hdf5):\n",
    "        # read texts from Asap file\n",
    "        input_file = pd.read_csv(file_csv)\n",
    "        essays = input_file['essay'].values\n",
    "        essay_id = np.array(input_file['essay_id'].values, dtype=np.uint16)\n",
    "        emb_list = []\n",
    "        \n",
    "        # Make a unique list of sentences to parallel embeddings\n",
    "        for essay in essays:\n",
    "            sentences = sent_tokenize(essay)\n",
    "            #Compute the embeddings using the multi-process pool\n",
    "            emb = model.encode_multi_process(sentences, pool)\n",
    "            # compute pool (mean the embeddings)\n",
    "            emb_pool = np.mean(emb, axis=0)\n",
    "            emb_list.append(emb_pool)\n",
    "        \n",
    "        # convert list to a to array\n",
    "        data = np.array(emb_list, dtype=np.float32)\n",
    "        print('shape of the array:', data.shape)\n",
    "        # write to the hdf5 file\n",
    "        f = h5py.File(file_hdf5, mode='r+')\n",
    "        f['/essay_id'] = essay_id\n",
    "        f['/data'] = data\n",
    "        f.close()\n",
    "\n",
    "        \n",
    "    #Optional: Stop the proccesses in the pool\n",
    "    model.stop_multi_process_pool(pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6ec43f0-b940-47cd-8d2e-86a127fb1ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1783,)\n",
      "[[-0.00922659  0.03124746 -0.01250991 ...  0.02161009 -0.02491724\n",
      "  -0.007745  ]\n",
      " [ 0.00144173  0.06095947 -0.01735124 ...  0.0249634  -0.01465576\n",
      "  -0.0198111 ]]\n",
      "(1800,)\n",
      "[[ 0.00910369  0.05059931 -0.00264957 ... -0.01627963  0.01587244\n",
      "  -0.00826657]\n",
      " [ 0.04705726  0.07408167 -0.00594867 ... -0.00919404 -0.00724361\n",
      "   0.00038601]]\n",
      "(1726,)\n",
      "[[ 0.00014497 -0.0193138  -0.03675896 ... -0.01913233  0.03073528\n",
      "  -0.01637933]\n",
      " [ 0.02078576 -0.04045704 -0.00054409 ...  0.00511276  0.00618818\n",
      "  -0.01427932]]\n",
      "(1772,)\n",
      "[[ 0.02182995  0.09105893 -0.0024663  ...  0.00200623  0.0395928\n",
      "  -0.01122126]\n",
      " [-0.02006244 -0.06909715 -0.02033951 ...  0.0136023   0.0234135\n",
      "  -0.03045519]]\n",
      "(1805,)\n",
      "[[-0.01462389  0.01998284 -0.01343488 ...  0.01645633  0.01985981\n",
      "  -0.01801151]\n",
      " [-0.04109019  0.03315708 -0.01877466 ...  0.01024659  0.01051008\n",
      "  -0.00861973]]\n",
      "(1800,)\n",
      "[[-0.0100621   0.01959658 -0.00715139 ...  0.00786291 -0.00249043\n",
      "  -0.00828326]\n",
      " [-0.0165115   0.00255447 -0.00583109 ... -0.01115719 -0.00768991\n",
      "  -0.01136737]]\n",
      "(1569,)\n",
      "[[ 0.06118718 -0.01628947 -0.00520502 ... -0.01789014  0.03552868\n",
      "  -0.01899207]\n",
      " [-0.01469846 -0.02525452  0.0056938  ...  0.02012222  0.02701505\n",
      "  -0.03650727]]\n",
      "(723,)\n",
      "[[ 0.02270852 -0.00342052 -0.01362963 ...  0.03568932 -0.01591185\n",
      "  -0.01981219]\n",
      " [-0.02893244  0.01340123 -0.00377719 ...  0.02716134  0.02355876\n",
      "  -0.01240314]]\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "for file_hdf5 in file_names_hdf5:\n",
    "    f = h5py.File(file_hdf5, mode='r+')\n",
    "    essay_dataset = f['/essay_id']\n",
    "    dataset = f['/data']\n",
    "    print(essay_dataset.shape)\n",
    "    print(dataset[:2])\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
